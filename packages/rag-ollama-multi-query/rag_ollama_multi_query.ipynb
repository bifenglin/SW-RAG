{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "681a5d1e",
   "metadata": {},
   "source": [
    "## Connect to template\n",
    "\n",
    "In `server.py`, set -\n",
    "```\n",
    "add_routes(app, chain_ext, path=\"/rag_ollama_multi_query\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d61a866-f91f-41ec-a840-270b0c9c895c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:12.947585Z",
     "start_time": "2024-05-09T09:21:12.688693Z"
    }
   },
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Server error '500 Internal Server Error' for url 'http://localhost:8000/rag-ollama-multi-query/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500 for Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/anaconda3/envs/graph/lib/python3.9/site-packages/langserve/client.py:157\u001B[0m, in \u001B[0;36m_raise_for_status\u001B[0;34m(response)\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 157\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/graph/lib/python3.9/site-packages/httpx/_models.py:761\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    760\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[0;32m--> 761\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m: Server error '500 Internal Server Error' for url 'http://localhost:8000/rag-ollama-multi-query/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangserve\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RemoteRunnable\n\u001B[1;32m      3\u001B[0m rag_app_ollama \u001B[38;5;241m=\u001B[39m RemoteRunnable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp://localhost:8000/rag-ollama-multi-query/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mrag_app_ollama\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat are the different types of agent memory?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/graph/lib/python3.9/site-packages/langserve/client.py:356\u001B[0m, in \u001B[0;36mRemoteRunnable.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs:\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkwargs not implemented yet.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_invoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/graph/lib/python3.9/site-packages/langchain_core/runnables/base.py:1625\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[0;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[1;32m   1621\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[1;32m   1622\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(var_child_runnable_config\u001B[38;5;241m.\u001B[39mset, child_config)\n\u001B[1;32m   1623\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m   1624\u001B[0m         Output,\n\u001B[0;32m-> 1625\u001B[0m         \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1626\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1627\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1628\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1629\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1630\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1631\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1632\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1633\u001B[0m     )\n\u001B[1;32m   1634\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1635\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/graph/lib/python3.9/site-packages/langchain_core/runnables/config.py:347\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    346\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 347\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/graph/lib/python3.9/site-packages/langserve/client.py:343\u001B[0m, in \u001B[0;36mRemoteRunnable._invoke\u001B[0;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Invoke the runnable with the given input and config.\"\"\"\u001B[39;00m\n\u001B[1;32m    335\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msync_client\u001B[38;5;241m.\u001B[39mpost(\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/invoke\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    337\u001B[0m     json\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    341\u001B[0m     },\n\u001B[1;32m    342\u001B[0m )\n\u001B[0;32m--> 343\u001B[0m output, callback_events \u001B[38;5;241m=\u001B[39m \u001B[43m_decode_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lc_serializer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m    345\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_server_callback_events \u001B[38;5;129;01mand\u001B[39;00m callback_events:\n\u001B[1;32m    348\u001B[0m     handle_callbacks(run_manager, callback_events)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/graph/lib/python3.9/site-packages/langserve/client.py:230\u001B[0m, in \u001B[0;36m_decode_response\u001B[0;34m(serializer, response, is_batch)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decode_response\u001B[39m(\n\u001B[1;32m    224\u001B[0m     serializer: Serializer,\n\u001B[1;32m    225\u001B[0m     response: httpx\u001B[38;5;241m.\u001B[39mResponse,\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m    227\u001B[0m     is_batch: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    228\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Any, Union[List[CallbackEventDict], List[List[CallbackEventDict]]]]:\n\u001B[1;32m    229\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Decode the response.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[43m_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m     obj \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/graph/lib/python3.9/site-packages/langserve/client.py:165\u001B[0m, in \u001B[0;36m_raise_for_status\u001B[0;34m(response)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext:\n\u001B[1;32m    163\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 165\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError(\n\u001B[1;32m    166\u001B[0m     message\u001B[38;5;241m=\u001B[39mmessage,\n\u001B[1;32m    167\u001B[0m     request\u001B[38;5;241m=\u001B[39m_sanitize_request(e\u001B[38;5;241m.\u001B[39mrequest),\n\u001B[1;32m    168\u001B[0m     response\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39mresponse,\n\u001B[1;32m    169\u001B[0m )\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m: Server error '500 Internal Server Error' for url 'http://localhost:8000/rag-ollama-multi-query/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500 for Internal Server Error"
     ]
    }
   ],
   "source": [
    "from langserve.client import RemoteRunnable\n",
    "\n",
    "rag_app_ollama = RemoteRunnable(\"http://localhost:8000/rag-ollama-multi-query/\")\n",
    "rag_app_ollama.invoke (\"What are the different types of agent memory?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='What a fascinating topic!\\n\\nThe history of Artificial Intelligence (AI) spans several decades, with roots in ancient civilizations and rapid progress in the 20th century. Here\\'s a brief overview:\\n\\n**Ancient Origins:**\\n\\n* The concept of artificial intelligence dates back to ancient Greece, where myths about creation of lifelike statues and automata existed.\\n* In the 2nd century AD, the Greek philosopher Plato wrote about the idea of creating artificial beings that could think and act like humans.\\n\\n**19th-20th Century Developments:**\\n\\n* In the late 19th century, Charles Babbage proposed the concept of a \"difference engine\" that could perform calculations automatically.\\n* In the early 20th century, Alan Turing\\'s theoretical work on the Universal Turing Machine laid the foundation for modern computer science and AI.\\n* The term \"Artificial Intelligence\" was coined in 1956 by John McCarthy, who also organized the first AI conference at Dartmouth College.\\n\\n**The Dawn of AI:**\\n\\n* In the 1950s, computers were developed that could perform simple tasks, such as playing tic-tac-toe and solving math problems.\\n* The first AI program, called Logical Theorist (LT), was created in 1956 by Allen Newell and Herbert Simon. LT could solve problems using logical reasoning.\\n* In the 1960s, the development of rule-based systems and expert systems further advanced AI research.\\n\\n**Rule-Based Systems:**\\n\\n* In the 1970s and 1980s, AI researchers focused on developing rule-based systems that could reason about knowledge bases.\\n* Expert systems, such as MYCIN (developed in 1986), were designed to mimic human expertise in specific domains.\\n\\n**Machine Learning and Neural Networks:**\\n\\n* The 1990s saw a resurgence of interest in neural networks, which were inspired by the workings of the human brain.\\n* Machine learning algorithms, such as decision trees and support vector machines (SVMs), were developed to enable AI systems to learn from data.\\n\\n**Modern AI:**\\n\\n* In the 2000s, AI research began to focus on machine learning, deep learning, and cognitive computing.\\n* The development of cloud computing and big data analytics enabled the creation of larger, more complex AI systems.\\n* Recent advancements in areas like natural language processing (NLP), computer vision, and reinforcement learning have led to breakthroughs in applications such as:\\n\\n1. Virtual assistants (e.g., Siri, Alexa)\\n2. Image recognition systems\\n3. Chatbots\\n4. Self-driving cars\\n\\n**Current State:**\\n\\nToday, AI is a rapidly growing field with numerous applications across industries, including healthcare, finance, education, and more.\\n\\nHowever, concerns about the impact of AI on society, job displacement, and ethics in AI development have also emerged as important areas for discussion.\\n\\nThe history of AI is marked by significant progress, challenges, and debates. As we continue to push the boundaries of what\\'s possible with AI, it\\'s essential to consider the ethical implications and potential consequences of these technologies.', response_metadata={'model': 'llama3:latest', 'created_at': '2024-05-09T06:01:37.812785Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 26324019084, 'load_duration': 5138948709, 'prompt_eval_count': 17, 'prompt_eval_duration': 147407000, 'eval_count': 625, 'eval_duration': 21034036000}, id='run-a9089c02-c364-4cf6-935f-86722b3fa118-0')"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "ollama_llm = \"llama3:latest\"\n",
    "llm = ChatOllama(model=ollama_llm)\n",
    "\n",
    "llm.invoke(\"Tell me about the history of AI\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T06:01:37.823045Z",
     "start_time": "2024-05-09T06:01:11.485475Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "graph",
   "language": "python",
   "display_name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
