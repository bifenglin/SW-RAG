{
    "answer": "Their model improves interpretability compared to softmax transformers by proposing an adaptive version of sparse attention where the shape of each attention head is learnable and can vary continuously and dynamically between the dense limit case of softmax and the sparse, piecewise-linear sparsemax case. This adaptivity allows for a more detailed analysis of attention head behavior, leading to the identification of both crisper examples observed in previous work and novel behaviors unraveled thanks to the sparsity and adaptivity of their proposed model. Additionally, the adaptively sparse Transformer can dynamically select sparsity patterns that find relevant words regardless of their position, contributing to a more interpretable model."
}